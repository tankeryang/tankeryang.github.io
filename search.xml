<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Machine Learning (Week4)]]></title>
    <url>%2Fposts%2FMachine-Learning%20(Week4)%2F</url>
    <content type="text"><![CDATA[第四周的课程开始介绍 Neural Networks - 神经网络 的概念，包括它的 结构 和 表示方法。 Neural Networks: Representation - 神经网络：表述 Non-linear hypotheses - 非线性假设 我们之前所讲到的，线性回归和逻辑回归，它们在进行 非线性预测和分类 时，在 特征比较少 的情况下，用 特征多项式 来训练会有不错的表现（如下式，两个特征）。 $\theta_{0} + \theta_{1}x_{1}^{2} + \theta_{2}x_{1}x_{2} + \theta_{3}x_{2}^{2}$ 但是一旦特征变多，比如几十甚至上百个时，利用特征多项式训练，所组合出来的新特征将会是一个非常庞大的数字（如下式，100个特征，仅两两组合）。这无疑是行不通的。 $\theta_{0} + \theta_{1}x_{1}^{2} + \theta_{2}x_{2}^{2} + \theta_{3}x_{3}^{2} + \cdots + \theta_{100}x_{100}^{2} + \theta_{101}x_{1}x_{2} + \theta_{102}x_{1}x_{3} + \cdots + \theta_{10000}x_{99}x_{100}$ 所以，我们需要一种新的模型，就是 神经网络。 Neurons and the brain - 神经元和大脑 这部分是介绍神经网络的一些背景，若不感兴趣可 点此跳至下一节 首先神经网络是模仿大脑结构来建立的。我们大脑可以学习很多事情，小到一个行为，一个动作，大到一门学科，一门语言，如果我们想让计算机来处理不同的学习任务，似乎我们需要针对性的编写不同的程序来实现。可是人脑在学习的时候真的用了这么多不同的“算法”么？我们能不能假设其实大脑只有一种学习算法，但却可以处理很多的事情？下面我们来看一下关于这个假设的一些证据。 科学家将小白鼠的 耳朵 到 听觉皮层 的 听觉神经 剪断，然后将 视觉神经 接到 听觉皮层 上，结果， 听觉皮层 学会了 看。 美国食品和药物管理局在临床实验一个名为brainport的系统，能帮助失明人士 看见 东西。它是这样工作的：在失明人士头上带一个灰度摄像头，获取面前场景的低分辨率灰度图像，然后将信号连接到 舌头 上的一个很薄的 电极阵列 上，这样 每个像素点都能映射到舌头的某个位置，这种系统能让人在几十分钟内用舌头学会 看东西。 这些例子说明，我们大脑的每一块区域，都能处理不同的信息，图像，声音，触觉，嗅觉等等，就好比一个机器可以接受任何传感器输入的信号。如果我们找出 大脑的学习方法，然后在计算机上实现，这也许是真正的迈向人工智能。而 神经网络 则是第一步。 下面我们将深入到神经网络的细节。 Model representation I - 模型表示 1 每一个神经元，也可以叫一个 计算节点，它接受来自 前一个神经元的输出 作为 它的输入，然后经过 计算，将 输出 送给 下一个神经元 作为其 输入。 graph LR; a1((a1)) -- input --> a2((a2)); a2((a2)) -- output --> a3((a3)); 比如一个以 逻辑回归 为模型的网络可以表示成这样： 这是一个只有 两层 的神经网络，只包含 输入层 和 输出层。 下面我们看一个3层的神经网络： 其中layer 2是 隐藏层，是中间的计算单元，将结果反馈到下一层。这里我们定义一下符号表示： $a_{i}^{(j)}$ 为第j层第i个节点 $h_{\Theta}(x)$ 为输出结果]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + NexT + Github Pages + Coding Pages + Gitee Pages + Travis 全攻略]]></title>
    <url>%2Fposts%2FHexo%20%2B%20NexT%20%2B%20Github%20Pages%20%2B%20Coding%20Pages%20%2B%20Gitee%20Pages%20%2B%20Travis%20%E5%85%A8%E6%94%BB%E7%95%A5%2F</url>
    <content type="text"><![CDATA[这几天刚更新了NexT主题，一直在修改细节，终于从5.1.x的版本更新到了6.0.x的版本，nodeJS和NPM也做了更新。本着 互联网共享精神，我在这里将 如何搭建Hexo+NexT博客和如何规范化写作+构建+push的流程 做详细整理。 本人现在常用Windows，所以下面的过程都是基于Windows来展开的，MacOS和Linux仅供参考。 安装Hexo 安装node.js 如果你已经安装了node.js，请忽略。 访问 node.js官网，下载安装程序（msi文件）进行安装。 安装Git 如果你已经安装了Git，请忽略。 访问 Git官网，下载安装程序（exe文件）进行安装。 由于众所周知的原因，从上面的链接下载git for windows最好挂上一个代理，否则下载速度十分缓慢。也可以参考 这个页面，收录了存储于百度云的下载地址。 安装Hexo 国内的朋友，因为众所周知的原因，从npm直接安装hexo会非常慢，所以你需要用到 镜像源 ，参考上面的步骤，使用cnpm命令行工具代替默认的npm: 在windows控制台（cmd）里输入并执行 npm install -g cnpm --registry=https://registry.npm.taobao.org，然后安装hexo: cnpm install -g hexo-cli 国外的朋友，请直接打开windows控制台，输入 npm install -g hexo-cli并执行。 建站 建立本地博客文件夹 在命令行执行如下命令，其中 &lt;folder&gt;为文件夹路径 12 hexo init &lt;folder&gt;cd &lt;folder&gt; 示例 12 hexo init C:/hexo/myblogcd C:/hexo/myblog 以下步骤均采用这个路径作为说明，并且 所有有关 hexo的命令 均要在此路径下执行。 建立好后文件夹目录如下 123456789 .├── _config.yml├── package.json├── .gitignore├── node_modules├── scaffolds├── source| ├── _posts└── themes 其中 _config.yml：博客的配置文件，可以在此配置大部分的参数。 package.json：应用程序的信息。EJS, Stylus和Markdown renderer 已默认安装，您可以自由移除。 package.json 12345678910111213141516171819 &#123; "name": "hexo-site", "version": "0.0.0", "private": true, "hexo": &#123; "version": "" &#125;, "dependencies": &#123; "hexo": "^3.2.0", "hexo-generator-archive": "^0.1.4", "hexo-generator-category": "^0.1.3", "hexo-generator-index": "^0.2.0", "hexo-generator-tag": "^0.2.0", "hexo-renderer-ejs": "^0.3.0", "hexo-renderer-stylus": "^0.3.1", "hexo-renderer-marked": "^0.3.0", "hexo-server": "^0.2.0" &#125;&#125; scaffolds：模板文件夹，当您新建文章时，Hexo会根据scaffold来建立文件。 source：资源文件夹，存放用户资源的地方。除 _posts文件夹之外，开头命名为 _ (下划线)的文件/文件夹和隐藏的文件将会被忽略。Markdown和HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。 themes：主题文件夹。Hexo会根据主题来生成静态页面。 node_modules：node.js模块，一些 插件 和 依赖 会被安装到这里。 更加详细的解释请参考 hexo官方文档 安装NexT主题 进入本地博客文件夹并将NexT主题 clone至 themes文件夹下 12 cd C:/hexo/mybloggit clone https://github.com/theme-next/hexo-theme-next themes/next 接着，进入 ./themes/next/文件夹，将 隐藏文件夹 .git删除。这一步是为了后面将网站源码push到github上的 必要工作。 你会看到，在 next下也有一个 _config.yml的文件，这是 NexT主题的配置文件，为了区别它和 博客配置文件，下面会用带路径的文件名来描述它们： myblog/_config.yml：博客配置文件 next/_config.yml：主题配置文件 启用NexT主题 在 myblog/_config.yml里 theme:选项填 next，=&gt; theme: next，注意冒号后空一格。 到这里，建站的任务就完成了。你现在可以打开控制台，输入并执行如下命令： 12 cd C:/hexo/mybloghexo g &amp;&amp; hexo s 其中 hexo g：新建 public文件夹，并在其中生成网站静态文件（html，css，等文件） hexo s：启动hexo服务器，默认情况下，访问网址为： http://localhost:4000/ 更多有关hexo的命令，请参考 hexo官方文档的 命令部分。 你会看到控制台有如下输出： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 INFO Start processingINFO Files loaded in 624 msINFO Generated: index.htmlINFO Generated: archives/index.htmlINFO Generated: images/algolia_logo.svgINFO Generated: images/cc-by-nc-nd.svgINFO Generated: images/cc-by-nc-sa.svgINFO Generated: images/avatar.gifINFO Generated: images/cc-by-nc.svgINFO Generated: images/apple-touch-icon-next.pngINFO Generated: images/cc-by-sa.svgINFO Generated: images/cc-by.svgINFO Generated: images/cc-zero.svgINFO Generated: images/cc-by-nd.svgINFO Generated: images/favicon-32x32-next.pngINFO Generated: images/favicon-16x16-next.pngINFO Generated: images/loading.gifINFO Generated: images/placeholder.gifINFO Generated: images/logo.svgINFO Generated: images/quote-r.svgINFO Generated: images/quote-l.svgINFO Generated: images/searchicon.pngINFO Generated: archives/2018/01/index.htmlINFO Generated: archives/2018/index.htmlINFO Generated: lib/font-awesome/HELP-US-OUT.txtINFO Generated: css/main.cssINFO Generated: lib/velocity/velocity.ui.min.jsINFO Generated: lib/velocity/velocity.min.jsINFO Generated: lib/font-awesome/bower.jsonINFO Generated: lib/velocity/velocity.jsINFO Generated: lib/velocity/velocity.ui.jsINFO Generated: lib/jquery/index.jsINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woffINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woff2INFO Generated: js/src/affix.jsINFO Generated: lib/ua-parser-js/dist/ua-parser.min.jsINFO Generated: js/src/bootstrap.jsINFO Generated: js/src/motion.jsINFO Generated: js/src/js.cookie.jsINFO Generated: js/src/exturl.jsINFO Generated: js/src/algolia-search.jsINFO Generated: js/src/post-details.jsINFO Generated: js/src/scrollspy.jsINFO Generated: lib/ua-parser-js/dist/ua-parser.pack.jsINFO Generated: lib/font-awesome/css/font-awesome.min.cssINFO Generated: js/src/utils.jsINFO Generated: js/src/scroll-cookie.jsINFO Generated: lib/font-awesome/css/font-awesome.css.mapINFO Generated: lib/font-awesome/css/font-awesome.cssINFO Generated: 2018/01/22/hello-world/index.htmlINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.eotINFO Generated: js/src/schemes/pisces.jsINFO 50 files generated in 865 msINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 在浏览器地址栏输入 http://localhost:4000/并访问，你应该会看到如下页面： &nbsp; 恭喜你！你已经完成了博客搭建的主要工作！接下来就是细节的配置了。请耐心阅读以下内容。 配置博客配置文件 整个 myblog/_config.yml的内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980 # Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Hexosubtitle:description:author: John Doelanguage:timezone:# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: 博客基础配置 这里是配置博客基础的地方，包括 博客名， 小标题， 描述， 站长名（你的昵称）， 语言， 时区。 1234567 # Sitetitle: # 博客名subtitle: # 小标题description: # 描述author: # 你叫啥language: # 语言timezone: # 时区 下面是我的配置： 1234567 # Sitetitle: 淦subtitle: n*m*lg(b)description: 汝亦知射乎？吾射不亦精乎？author: tankeryanglanguage: zh-Hanstimezone: 你可以参考一下 哪项配置分别对应哪个位置，其中 language: zh-Hans这里是根据 主题是否支持 来设置的，因为渲染的js和css等文件都在主题里。NexT主题支持的语言 参考这里。hexo默认使用您计算机设置的时区。更改时区请参考 时区列表，比如如果您想换成 纽约时区，您需填 America/New_York。 注意，您在查看NexT支持的语言时访问的是 v5.x的 NexT使用文档，阅读时请注意。 v6.x的github地址在 这里，官网在 这里 博客url配置 这里是配置你的博客 链接格式 的，包括 主站 和 文章 链接。 123456 # URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://yoursite.com # 博客链接root: / # 博客根目录permalink: :year/:month/:day/:title/ # 文章的永久链接格式permalink_defaults: # 永久链接中各部分的默认值 这是我的配置： 1234 url: https://tankeryang.github.ioroot: /permalink: posts/:title/permalink_defaults: 这里我的博客链接是 https://tankeryang.github.io，因为我是挂在github上的。 root设置为 /。如果你的博客是在 子目录 下，如 http://yoursite.com/child，你需这样设置： 12 url: http://yoursite.com/childroot: /child/ 接着是 permalink的配置，hexo默认的是 :year/:month/:day/:title/的格式。比如我点开博客搭建好之后的默认博文 Hello World，它的链接是这样的： http://localhost:4000/2018/01/22/hello-world/： 如果你想 更改文章永久链接格式 的话，以下是和链接格式有关的变量，你可以根据以下变量来配置： 变量 描述 :year 文章的发表年份（4 位数） :month 文章的发表月份（2 位数） :i_month 文章的发表月份（去掉开头的零） :day 文章的发表日期 (2 位数) :i_day 文章的发表日期（去掉开头的零） :title 文件名称 :id 文章 ID 更多有关链接的配置，请参考 hexo官方文档中的 永久链接部分。 &nbsp; 以下内容更新于2018/1/24，承接上文 配置资源文件夹 资源（Asset）代表source文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在 source/images 文件夹中。然后通过类似于 ![](/images/image.jpg)的方法访问它们。 如果你想要更有规律地提供图片和其他资源以及想要将他们的资源分布在各个文章上的人来说，Hexo也提供了更组织化的方式来管理资源。这个稍微有些复杂但是管理资源非常方便的功能可以通过将 config.yml文件中的 post_asset_folder选项设为 true来打开： 1 post_asset_folder: true # 设置为true 设置为 true后，当你新建一篇文章时，hexo同时会新建一个 和文章标题一样名字 的文件夹，你的文章所引用的图片等资源就可以放在这里面了。 将所有与你的文章有关的资源放在这个关联文件夹中之后，你可以通过 标签插件 来引用它们，这样你就得到了一个更简单而且方便得多的工作流。关于什么是标签插件，接下来的内容会说明。请耐心阅读。（你也可以点击上面的链接浏览一下标签插件的内容） &nbsp; 以下内容更新与2018/1/25，承接上文（不好意思，本人很懒…） 写作 新建文章 执行如下命令新建文章： 1 hexo new [layout] &lt;title&gt; 其中 [layout]字段是文章的 布局，默认为 post，可以通过修改 _config.yml中的 default_layout参数来指定默认布局。 &lt;title&gt;则是文章标题。 布局 Hexo 有三种默认布局： post、 page和 draft，它们分别对应不同的路径，而您 自定义的其他布局 和 post相同，都将储存到 source/_posts文件夹。 布局 路径 post source/_posts page source draft source/_draft 其实布局我到现在也不是很清楚是什么， 我是这样认为的： 如果你执行了这条命令 hexo new post &quot;new article&quot;，hexo会新建一个 new article.md文件在 source/_post文件夹下。同理，其他两个布局参照上面的路径新建文章。 文件名称 Hexo默认以 标题 作为文件名称，你也可编辑 myblog/_config.yml中的 new_post_name参数来改变默认的文件名称，比如设置为： 1 new_post_name: :year-:month-:day-:title.md # File name of new posts 这样在 文章名 前面就会加上日期和时间共同组成 文件名。 下面是一些可用来配置文件名的变量： 变量 描述 :title 标题（小写，空格将会被替换为短杠） :year 建立的年份,比如，2015 :month 建立的月份（有前导零），比如， 04 i_month 建立的月份（无前导零），比如， 4 :day 建立的日期（有前导零），比如， 07 i_day 建立的日期（无前导零），比如， 7 更多有关 基本写作设置 的内容请参考 hexo官方文档的 写作部分。 下面，我们就来尝试一下吧！ 首先，执行 1 hexo new post "caonima" 你会看到输出 1 INFO Created: C:/hexo/myblog/source/_posts/caonima.md 同时，在 source/_post文件夹下多了 caonima.md文件和 caonima文件夹。 1234 .├── caonima.md├── hello-world.md├── caonima 打开 caonima.md，你会看到这些 12345 ---title: caonimadate: 2018-01-25 13:06:28tags:--- 这些是 front-matter，下面我会对它进行说明，请耐心阅读（ 这句话我到底讲了几次）。 接下来你可以随便在里面写点内容，比如： 123456789101112 # caonima## caonima### caonima__caonima___caonima_* caonima&gt; caonima[caonima]()~~caonima~~`caonima` 保存。执行下面语句生成博客文件并运行本地hexo服务端： 1 hexo g &amp;&amp; hexo s 你会看到如下输出： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 INFO Start processingINFO Files loaded in 642 msINFO Generated: 2018/01/22/hello-world/index.htmlINFO Generated: archives/index.htmlINFO Generated: index.htmlINFO Generated: archives/2018/01/index.htmlINFO Generated: archives/2018/index.htmlINFO Generated: images/avatar.gifINFO Generated: images/apple-touch-icon-next.pngINFO Generated: images/cc-by-nc-nd.svgINFO Generated: images/algolia_logo.svgINFO Generated: images/cc-by-nc-sa.svgINFO Generated: images/cc-by-nd.svgINFO Generated: images/cc-by-nc.svgINFO Generated: images/cc-by.svgINFO Generated: images/favicon-16x16-next.pngINFO Generated: images/cc-zero.svgINFO Generated: images/favicon-32x32-next.pngINFO Generated: images/cc-by-sa.svgINFO Generated: images/logo.svgINFO Generated: images/placeholder.gifINFO Generated: images/quote-l.svgINFO Generated: images/loading.gifINFO Generated: images/quote-r.svgINFO Generated: lib/font-awesome/HELP-US-OUT.txtINFO Generated: lib/font-awesome/css/font-awesome.css.mapINFO Generated: images/searchicon.pngINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woff2INFO Generated: lib/font-awesome/fonts/fontawesome-webfont.woffINFO Generated: js/src/algolia-search.jsINFO Generated: js/src/exturl.jsINFO Generated: js/src/affix.jsINFO Generated: js/src/post-details.jsINFO Generated: js/src/motion.jsINFO Generated: js/src/scroll-cookie.jsINFO Generated: js/src/utils.jsINFO Generated: js/src/scrollspy.jsINFO Generated: lib/font-awesome/bower.jsonINFO Generated: js/src/js.cookie.jsINFO Generated: js/src/bootstrap.jsINFO Generated: lib/velocity/velocity.ui.min.jsINFO Generated: lib/ua-parser-js/dist/ua-parser.pack.jsINFO Generated: js/src/schemes/pisces.jsINFO Generated: lib/ua-parser-js/dist/ua-parser.min.jsINFO Generated: css/main.cssINFO Generated: lib/jquery/index.jsINFO Generated: lib/velocity/velocity.ui.jsINFO Generated: lib/font-awesome/css/font-awesome.cssINFO Generated: lib/velocity/velocity.min.jsINFO Generated: lib/font-awesome/css/font-awesome.min.cssINFO Generated: lib/velocity/velocity.jsINFO Generated: lib/font-awesome/fonts/fontawesome-webfont.eotINFO Generated: 2018/01/25/caonima/index.htmlINFO 51 files generated in 1 sINFO Start processingINFO Hexo is running at http://localhost:4000/. Press Ctrl+C to stop. 在浏览器输入 http://localhost:4000/并访问，你会看到这样的页面： &nbsp; 恭喜你！你已经可以进行简单的文字创作了！下面的任务就是让你的写作流程规范化的细节。请耐心阅读。 标签插件 从上方跳转过来的朋友， &nbsp;点此返回 资源配置文件夹处 标签插件是用于在文章中快速插入特定内容的插件。 它的在文章中用法一般是这样： 123 &#123;% [某种标签] %&#125;&lt;你想插入的内容&gt;&#123;% end[某种标签] %&#125; 或者这样： 1 &#123;% [某种标签] &lt;你想插入的内容&gt; %&#125; 用例子说明最快： 比如像前面提到的， 用标签插件在文章引用图片，你只需这样写 1 &#123;% asset_img &lt;图片文件名&gt; %&#125; 示例 1 &#123;% asset_img caonima.jpg %&#125; 这是我最常用的标签了， asset_img，顾名思义，就是图片资源。一般我都用它来插入图片。因为我们在前面配置了 资源文件夹，所以 &lt;图片文件名&gt;这里我们不用输入绝对路径， 只需输入图片文件名 就ok了， hexo会自动在资源文件夹里寻找你的图片。 你可以在 caonima文件夹里放一张图片，然后在 caonima.md里用上面的 asset_img标签插件来引用它，看看效果。 更多有关标签插件的内容，请参考 hexo官方文档中的 标签插件部分。 配置主题配置文件 NexT主题作为hexo众多主题里最火的一款，除了简约美观的设计之外，最重要的一点就是 可定制化的程度高。你可以很轻松的 开启或关闭某些功能，甚至 自己尝试添加一些功能也比其他主题简单，因为它的 源文件组织得很清晰，主题的 布局， js， css， 字体， 语言，等文件都独立区分。 下面我会参照我的配置来详细介绍如何配置NexT主题。 网站图标 下面就是网站图标的配置项： 123456789 # For example, you put your favicons into `hexo-site/source/images` directory.# Then need to rename &amp; redefine they on any other names, otherwise icons from Next will rewrite your custom icons in Hexo.favicon: small: /images/favicon-16x16-next.png medium: /images/favicon-32x32-next.png apple_touch_icon: /images/apple-touch-icon-next.png safari_pinned_tab: /images/logo.svg #android_manifest: /images/manifest.json #ms_browserconfig: /images/browserconfig.xml 参照注释，先在 myblog/source/路径下新建 images文件夹，找一张 16x16的 ico或者 png图标，放进 images文件夹（在哪里找图标请自行百度），比如 caonima.ico。 然后将 small选项设置为 /images/caonima.ico： 12 favicon: small: /images/caonima.ico 再将其他的选项注释掉（因为基本用不到）： 1234567 favicon: small: /images/caonima.ico #medium: /images/favicon-32x32-next.png #apple_touch_icon: /images/apple-touch-icon-next.png #safari_pinned_tab: /images/logo.svg #android_manifest: /images/manifest.json #ms_browserconfig: /images/browserconfig.xml 网站图标配置就完成了。关于其他选项你可以有空自己放些图标文件来玩玩看什么效果。 网站底部内容 这些在 footer选项的配置里： 12345678910111213141516171819202122 footer: # Specify the date when the site was setup. # If not defined, current year will be used. #since: 2015 # Icon between year and copyright info. icon: user # If not defined, will be used `author` from Hexo main config. copyright: # ------------------------------------------------------------- # Hexo link (Powered by Hexo). powered: true theme: # Theme &amp; scheme info link (Theme - NexT.scheme). enable: true # Version info of NexT after scheme info (vX.X.X). version: true # ------------------------------------------------------------- # Any custom text can be defined here. #custom_text: Hosted by &lt;a target="_blank" rel="external nofollow" href="https://pages.coding.me"&gt;&lt;b&gt;Coding Pages&lt;/b&gt;&lt;/a&gt; &nbsp;未完待续…明天继续…]]></content>
      <categories>
        <category>技术杂项</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>NexT</tag>
        <tag>Github Pages</tag>
        <tag>Coding Pages</tag>
        <tag>Gitee Pages</tag>
        <tag>Travis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[COS站半次元原图爬虫 banciyuan-downloader v1.0 发布]]></title>
    <url>%2Fposts%2FCOS%E7%AB%99%E5%8D%8A%E6%AC%A1%E5%85%83%E5%8E%9F%E5%9B%BE%E7%88%AC%E8%99%AB-banciyuan-downloader-v1-0%20%E5%8F%91%E5%B8%83%2F</url>
    <content type="text"><![CDATA[请勿用于商业用途 尊重coser的版权 转载图片请注明Cn及链接 今天突然看到一组 《小魔女学院》的cos图，小姐姐美炸了。 &nbsp;因为加载速度太慢，我只能调低了图片的分辨率，各位将就一下，想看原图的到上面的链接慢慢欣赏。 于是乎搜到了这位小姐姐 【cn:犬神洛洛子】 cos粉毛那位 这是她的 半次元主页 关注一波。想着有空写个爬虫来爬她的原图吧。结果越写添加的功能越多。现在这个版本我push到了 github，上面有详细的使用方法。 目前版本支持功能情况 根据 coser_id批量下载某个coser发布的主题的所有图片 图片保存在以 coser名 命名的文件夹内 图片按 coser发布的主题 分文件夹保存，文件夹以主题标题命名 若有相同标题的主题，则命名文件夹时会加上随机后缀防止文件名冲突 图片命名格式为 %num%.jpg/ %num%.png，其中 %num%为从 1开始的 编号 只下载 最新 发布的图片，本地已有的 不会 重复下载 支持 断点续传 （从断连主题的下一个主题开始下载，若断连主题没下载完，则会丢失一部分断连主题的图片，其余均不影响） 无须提供半次元账号即可下载 下载指定主题 智能下载 未下载过的主题 无须提前关注就可下载 粉丝可见的主题 运行环境及python包版本（本人） windows 10 1703-15063.674 python 3.6.1 beautifulsoup 4.5.3 requests 2.13.0 lxml 3.7.2 运行结果 FAQ 有何疑问可在 github发布 issue ，本人会尽量及时查看 疯狂打call 原图出处: 小魔女学园-主角三人搞事组cos 苏西·曼芭芭拉(粉毛) cn : 犬神洛洛子 亚可·卡嘉莉(黑毛) cn :real__yami 洛蒂·杨森(黄毛) cn : 樱群]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>二次元</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning (Week3)]]></title>
    <url>%2Fposts%2FMachine%20Learning%20(Week3)%2F</url>
    <content type="text"><![CDATA[在第三周的课程里，介绍了 Logistic Regression - 逻辑斯谛回归问题，主要应用在 Classification - 分类上。还有 Regularization - 正则化，如何用来解决 Overfitting - 过拟合问题。 Logistic Regression - 逻辑斯谛回归 Classification - 分类问题 分类在日常中用在很多地方，比如邮件是否垃圾邮件，肿瘤是否良性。通过给定的特征与对应的类别，我们可以训练出一个能够进行分类的算法。相应的，垃圾邮件的特征可以是某些关键词，比如推销类的；而肿瘤的特征可以是肿瘤大小或者别的什么（不懂就不胡说了）。 这时我们的结果$y$就是离散化的数字。 如果是 Binary Classification - 二分类问题，$y$可以离散化为$y = 0 or 1$，对应 是/否， 大/小等抽象的结果。 如果是 Multiple Classification - 多分类问题，$y$可以离散化为 元素个数为类别个数的向量。比如我们需要对某组数据进行分类，训练样本中一共有$n$类，当前训练样本的$y$是属于第$i$类的，则令 $y = [0_{1},0_{2},\cdots,1_{i},\cdots,0_{n}]^{T}$ ，其中下标位置为对应类别。 这里我们先讨论 Binary Classification - 二分类问题。同样先来个例子。 假如我们有一组肿瘤大小与其对应性质（良性/恶性）的数据，特征只有一个，就是肿瘤大小，$y=1$和 红色X点代表恶性。如下图： 我们看到，假如我们用单纯的线性方程来拟合这组数据，按照图示定义，当 $h_{\theta} = \theta^{T}x &gt;0.5$ 时预测为恶性，则会与原数据 误差较大。显然单纯的线性方程很难做到精准的分类。 我们尝试换一种思路。可以看到，两种类型的数据在某一$x$值上会有明显的区分。在$x$左边是良性的，在$x$右边是恶性的。于是我们做如下分析： 令分界点 $x = \frac{-\theta_{0}}{\theta_{1}}$ ，则当 $x &gt; \frac{-\theta_{0}}{\theta_{1}}$ 即 $\theta^{T}x = \theta_{0}+\theta_{1}x &gt; 0$ 时，预测$y=1$，当 $x &lt; \frac{-\theta_{0}}{\theta_{1}}$ 即 $\theta^{T}x = \theta_{0}+\theta_{1}x &lt; 0$ 时，预测$y=0$。这样就能得到很好的分类效果。 同样的，多特征时也可以如此这般。比如两个特征时的情况： 因此我们只要一个函数 $g(\theta^{T}x)$ 当 $\theta^{T}x &gt; 0$ 时， $g(\theta^{T}x) &gt; 0.5$ 当 $\theta^{T}x &lt; 0$ 时， $g(\theta^{T}x) &lt; 0.5$ 最后令 $h_{\theta}(x) = g(\theta^{T}x)$ ，就ok了。 Hypothesis Representation - 假设函数的表示 接着上面的问题。我们给出这样一个函数 $g(z) = \frac{1}{1+e^{-z}}$ 它的图像如下 它满足下述性质 当 $z &gt; 0$ 时， $g(z) &gt; 0.5$ 当 $z &lt; 0$ 时， $g(z) &lt; 0.5$ 因此，我们只需令 $\theta^{T}x = z$ ，即 $g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}$ 则可得我们的假设函数 $h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}$ 这里 $h_{\theta}(x)$ 实际上可以理解为如下 $h_{\theta}(x) = p(y = 1|x;\theta)$ 即输入 $x$ 的情况下，预测结果为 $1$ 的概率为 $h_{\theta}(x)$ 。 比如说这是一个良性/恶性肿瘤的分类问题，我们训练出了 $h_{\theta}(x)$ 。现在有一个肿瘤的各项特征为 $x$ ，我们要判断它是良性 $(y=1)$ 还是恶性 $(y=0)$ ，把 $x$ 丢进 $h_{\theta}(x)$ 里，结果为 $0.8$ ，那么我们就可以说这个肿瘤有 $80\%$ 的概率是良性的。假如我们设置了一个 阈值为 $0.7$ ，计算结果超过这个阈值就可以声明预测为真，那么在上面的情况中，我们就可以直接对病人说你的肿瘤是良性的，不用担心。 Decision Boundary - 判定边界 在逻辑斯谛回归中，我们一般 当 $h_{\theta}(x) &gt; 0.5$ ，即当 $\theta^{T}x &gt; 0$ 时，预测 $y=1$ 当 $h_{\theta}(x) &lt; 0.5$ ，即当 $\theta^{T}x &lt; 0$ 时，预测 $y=0$ 如下图（见上节） 此时 阈值为 $0.5$ 对于一般的问题， $0.5$ 的阈值足以胜任。可是假如是一些精确度要求很高的问题，就比如刚刚的判断肿瘤是否良性，或者判断是否得癌症等，那么就应该把 阈值设置得高点，预测结果更准确。毕竟是人命关天的事嘛:) 假如我们的样本分布不能线性划分，如下图所示 我们可以用一个非线性的边界（高次多项式）来划分。 Cost function - 代价函数 对于前面的回归问题，我们的代价函数是 所有误差的平方和取均值（实际上就是 方差） $J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{( i)}\right)^{2}$ 如果我们沿用这个计算方法，将逻辑斯谛回归的 $h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}$ 代入进上式的话，我们的函数图像会呈现出下面一种状况 这是一个 非凸函数，它有许多的局部最小值，不利于用梯度下降法寻找全局最小值。 所以我们要对逻辑回归重新定义一个代价函数。 根据我们 $h_{\theta}(x)$ 的性质 当 $\theta^{T}x &gt; 0$ 时， $h_{\theta}(x) &gt; 0.5$ 当 $\theta^{T}x &lt; 0$ 时， $h_{\theta}(x) &lt; 0.5$ $h_{\theta}(x) \in (0, 1)$ 我们对代价函数作出如下定义 $$Cost\left( h_{\theta}\left( x \right), y \right) =\begin{cases} -log\left( h_{\theta}\left( x \right ) \right )&amp; \text{ if } y = 1 \\ -log\left( 1 - h_{\theta}\left( x \right ) \right )&amp; \text{ if } y = 0 \end{cases}$$ 它的函数图像和意义如下所示 当 $y = 1$ 时 它所反映的就是当样本结果 $y = 1$ 时，如果我们 $h_{\theta}(x)$ 输出也 $\rightarrow 1$ 的话，我们的误差就 $\rightarrow 0$ ；反之，如果我们 $h_{\theta}(x)$ 输出 $\rightarrow 0$ 的话，我们的误差就 $\rightarrow \infty$ 当 $y = 0$ 时 它所反映的就是当样本结果 $y = 0$ 时，如果我们 $h_{\theta}(x)$ 输出也 $\rightarrow 0$ 的话，我们的误差就 $\rightarrow 0$ ；反之，如果我们 $h_{\theta}(x)$ 输出 $\rightarrow 1$ 的话，我们的误差就 $\rightarrow \infty$ 没毛病:) Simplified cost function and gradient descent - 化简代价函数与梯度下降 对于 $Cost\left( h_{\theta}\left( x \right), y \right)$ ，我们可以化简成 $Cost\left( h_{\theta}\left( x \right), y \right) = -ylog\left( h_{\theta}\left( x \right) \right) - \left( 1-y \right)log\left( 1-h_{\theta}\left( x \right) \right)$ 对于 $J(\theta)$ 我们作如下定义 $J(\theta) = \frac{1}{m} \sum_{i=1}^{m}Cost\left( h_{\theta}\left( x^{\left( i \right)} \right), y^{\left( i \right) }\right)$ 将 $Cost\left( h_{\theta}\left( x \right), y \right)$ 代入上式，则可得 $J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} \left[ y^{\left( i \right)}log\left( h_{\theta}\left( x^{\left( i \right)} \right) \right) + \left( 1-y^{\left( i \right)} \right)log\left( 1-h_{\theta}\left( x^{\left( i \right)} \right) \right) \right]$ 这时我们就可以用 梯度下降来求 $min_{\theta}J(\theta)$ 。 与回归一样，我们要做的就是不断更新 $\theta$ $\theta := \theta - \alpha \frac{\partial J}{\partial \theta}$ 接下来我们来推导一下 $\frac{\partial J}{\partial \theta}$ 。 首先，我们有 $X_{m \times (n+1)}\begin{bmatrix}x_{0}^{(1)} &amp; x_{1}^{(1)} &amp; \cdots &amp; x_{n}^{(1)}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ x_{0}^{(m)} &amp; x_{1}^{(m)} &amp; \cdots &amp; x_{n}^{(m)}\end{bmatrix} = \begin{bmatrix}1 &amp; x_{1}^{(1)} &amp; \cdots &amp; x_{n}^{(1)}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ 1 &amp; x_{1}^{(m)} &amp; \cdots &amp; x_{n}^{(m)}\end{bmatrix}$ $Y_{m\times 1} = \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)}\end{bmatrix}^{T}$ $\theta = \begin{bmatrix}\theta_{0} &amp; \theta_{1} &amp; \cdots &amp; \theta_{n} \end{bmatrix}^{T}$ $\frac{\partial J}{\partial \theta} = - \frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)}\frac{\partial log(h_{\theta}(x^{(i)}))}{\partial \theta} + ( 1-y^{(i)})\frac{\partial log(1-h_{\theta}( x^{(i)} ))}{\partial \theta} \right]$ $h_{\theta}(x^{(i)}) = g(\theta^{T}x^{(i)}) = \frac{1}{1+e^{-\theta^{T}x^{(i)}}}$ 因为 $y^{(i)} \frac{\partial log(h_{\theta}(x^{(i)}))}{\partial \theta} = \frac{y^{(i)}}{h_{\theta}(x^{(i)})} \cdot \frac{x^{(i)}e^{-\theta^{T}x^{(i)}}}{(1+e^{-\theta^{T}x^{(i)}})^{2}}$ $= y^{(i)}(1+e^{-\theta^{T}x^{(i)}}) \cdot \frac{x^{(i)}e^{-\theta^{T}x^{(i)}}}{(1+e^{-\theta^{T}x^{(i)}})^{2}}$ $= y^{(i)} \frac{x^{(i)}e^{-\theta^{T}x^{(i)}}}{1+e^{-\theta^{T}x^{(i)}}}$ $= y^{(i)} h_{\theta}(x^{(i)})x^{(i)}e^{-\theta^{T}x^{(i)}}$ 又因为 $(1-y^{(i)}) \frac{\partial log(1-h_{\theta}( x^{(i)} ))}{\partial \theta} = - (1-y^{(i)}) \frac{1}{1-h_{\theta}(x^{(i)})} \cdot \frac{x^{(i)}e^{-\theta^{T}x^{(i)}}}{(1+e^{-\theta^{T}x^{(i)}})^{2}}$ $= - (1-y^{(i)}) \frac{1+e^{-\theta^{T}x^{(i)}}}{e^{-\theta^{T}x^{(i)}}} \cdot \frac{x^{(i)}e^{-\theta^{T}x^{(i)}}}{(1+e^{-\theta^{T}x^{(i)}})^{2}}$ $= - (1-y^{(i)}) \frac{x^{(i)}}{1+e^{-\theta^{T}x^{(i)}}}$ $= - (1-y^{(i)})x^{(i)}h_{\theta}(x^{(i)})$ 将上面两式相加，提出 $h_{\theta}(x^{(i)})x^{(i)}$ ，得 $h_{\theta}(x^{(i)})x^{(i)} \left[y^{(i)}(1+e^{-\theta^{T}x^{(i)}}) - 1\right]$ $= x^{(i)}y^{(i)} - x^{(i)}h_{\theta}(x^{(i)})$ $= x^{(i)} (y^{(i)} - h_{\theta}(x^{(i)}))$ 将上式代入 $\frac{\partial J}{\partial \theta}$ ，则 $\frac{\partial J}{\partial \theta} = \frac{1}{m} \sum_{i=1}^{m}(h_{\theta}(x^{(i)}) - y^{(i)})x^{(i)}$ 我们惊奇地发现， $\frac{\partial J}{\partial \theta}$ 的表达式居然跟回归是一样的！这就是数学的魅力！ 因此，参考 week1的推导，可得 $\frac{\partial J}{\partial \theta} = \frac{1}{m} X^{T}(X\theta - Y)$ $$\theta := \theta - \alpha \frac{1}{m} X^{T}(X\theta - Y)$$ Multi-class classification: One-vs-all - 多分类问题：一对多 在实际分类问题中，我们遇到的大多是需要 分多个类的问题，比如联系人的分类有家人，朋友，同事，同学等等。在可视化图像中，它们可能会呈现出如下的分布 一对多的做法就是我们分别对这三个类训练 $h_{\theta}^{(i)}(x)$ ，其中 $i$ 为类别序号。如下图所示 训练完所有的 $h_{\theta}^{(i)}(x)$ 后，当我们给一组输入特征 $x$ 时，取 最大的 $h_{\theta}^{(i)}(x)$ 作为我们的预测结果。 Regularization - 正则化 The problem of overfitting - 过拟合问题 假如我们样本有非常多的特征，我们也许能训练出一个在样本集上表现得很好的假设函数 $h_{\theta}(x)$ ，但是对于新的输入，我们可能不能很好地进行拟合（预测）。这类问题，我们称之为 过拟合。 对于过拟合问题我们一般有下面一些解决方法 减少特征数量 手动剔除一些不必要的特征，或者用一些降维算法（PCA）来自动减少特征数 正则化 保留所有的特征，同时减小参数 $\theta$ 的大小 Cost function - 代价函数 首先看下面这两种预测函数在样本集上的结果 我们能看到，左边是比较合适的预测函数，而右边则明显过拟合了。 这时我们用一个小小的技巧，在我们的误差函数 $J(\theta)$ 后面对 $\theta_{3}$ 和 $\theta_{4}$ 加一个 惩罚系数（或者说补偿反馈），使之变为 $J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2} + 1000\theta_{3} + 1000\theta_{4}$ 由上可知，我们要求最优的 $\theta$ ，使得 $J(\theta)$ 取得最小值，那么我们在优化过程中（比如梯度下降） $\theta_{3}$ 和 $\theta_{4}$ 一定 $\rightarrow 0$ ，因为他们占比很大。这样 $\theta_{3}$ 和 $\theta_{4}$ 对 $h_{\theta}(x)$ 的贡献就非常小， $x^{3}$ 和 $x^{4}$ 这些高次项在 $h_{\theta}(x)$ 所占的权重就小很多，有效地防止了 过拟合。 假如我们有非常多的特征，不知道要对哪些对应的参数 $\theta$ 作惩罚，那么最好的办法就是对所有的 $\theta$ 作惩罚，然后让程序自己迭代优化。所以我们的代价函数 $J(\theta)$ 就变成下面这种形式 $J(\theta) = \frac{1}{2m} \left[ \sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2} + \lambda \sum_{j=1}^{n} \theta_{j}^{2} \right]$ 其中 $\lambda$ 称为 正则化参数。一般我们不对 $\theta_{0}$ 进行惩罚。 正则化后的假设函数如下图所示 其中 蓝色曲线是过拟合的情况， 紫色曲线是正则化后的假设函数曲线，而 橙色直线则是 正则化参数过大 导致的 欠拟合。为什么会这样呢？因为正则化参数过大，会对 $(\theta_{1} \cdots \theta_{n})$ 惩罚过重，以至于 $(\theta_{1} \cdots \theta_{n}) \rightarrow 0$ ，使得 $h_{\theta}(x) \approx \theta_{0}$ 。 因此对于正则化，我们要选一个合适的值，才有好的效果。 Regularized linear regression - 正则化后的线性回归 正则化后我们的代价函数变成 $J(\theta) = \frac{1}{2m} \left\{ \left[\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2} \right] + \lambda \sum_{j=1}^{n} \theta_{j}^{2} \right\}$ 如果我们用梯度下降来求最优 $\theta$ ，我们更新 $\theta$ 就要分别更新 $\theta_{0}$ 和 $\theta_{1} \cdots \theta_{n}$ $$\begin{cases} \theta_{0} := \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m}( h_{\theta}(x^{(i)})-y^{(i)} )x_{0}^{(i)} \\ \theta_{j} := \theta_{j} - \alpha \left\{ \frac{1}{m} \left[\sum_{i=1}^{m}( h_{\theta}(x^{(i)})-y^{(i)} )x_{j}^{(i)} \right]+\frac{\lambda}{m}\theta_{j}\right\} &amp; j=1,2, \cdots ,n \end{cases}$$ 其中 $\theta_{j}$ 可以化简成 $\theta_{j}(1-\alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}$ 我们看到， $(1-\alpha\frac{\lambda}{m}) &lt; 1$ ，所以正则化后的梯度下降实际上就是让 $\theta_{j}$ 减少一定的比例后再进行原来的梯度下降。 我们知道，梯度 $\frac{\partial J}{\partial \theta}$ 为 $0$ 时， $J(\theta)$ 取得极小值，所以我们令 $$\begin{cases} \sum_{i=1}^{m}( h_{\theta}(x^{(i)})-y^{(i)} )x_{0}^{(i)} = 0\\ \sum_{i=1}^{m}( h_{\theta}(x^{(i)})-y^{(i)} )x_{j}^{(i)} + \lambda\theta_{j} = 0 &amp; j=1,2, \cdots ,n \end{cases}$$ 即 $\frac{\partial J}{\partial \theta} =\begin{bmatrix}x_{0}^{(1)} &amp; x_{0}^{(2)} &amp;\cdots &amp; x_{0}^{(m)}\\ x_{1}^{(1)} &amp; x_{1}^{(2)} &amp; \cdots &amp; x_{1}^{(m)}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{n}^{(1)} &amp; x_{n}^{(2)} &amp; \cdots &amp; x_{n}^{(m)}\end{bmatrix} \begin{bmatrix}h_{\theta}(x^{(1)})-y^{(1)}\\h_{\theta}(x^{(2)})-y^{(2)}\\ \vdots\\h_{\theta}(x^{(m)})-y^{(m)}\end{bmatrix} + \lambda \begin{bmatrix}0\\ \theta_{1}\\ \vdots \\ \theta_{n}\end{bmatrix} = 0$ 也即 $X^{T}(X\theta - Y) + \lambda \begin{bmatrix}0 &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; 1\end{bmatrix}_{(n+1)^{2}} \theta = 0$ 去掉括号，并提出 $\theta$ ，整理等式 $(X^{T}X + \lambda\begin{bmatrix}0 &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; 1\end{bmatrix}_{(n+1)^{2}}) \theta = X^{T}Y$ 最后我们可得 $\theta = (X^{T}X + \lambda\begin{bmatrix}0 &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; 1\end{bmatrix}_{(n+1)^{2}})^{-1} X^{T}Y$ 上式就是正则化后的 Normal equation - 正规方程，其中 $(X^{T}X + \lambda\begin{bmatrix}0 &amp; &amp; &amp; \\ &amp; 1 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; 1\end{bmatrix}_{(n+1)^{2}})$ 一定是可逆的。这个就不在此作证明了。 Regularized logistic regression - 正则化后的逻辑斯谛回归 与线性回归一样，我们在原来的代价函数 $J(\theta)$ 后面加上一个惩罚项，则 $J(\theta)$ 变成 $J(\theta) = - \left\{ \frac{1}{m} \sum_{i=1}^{m} \left[ y^{\left( i \right)}log\left( h_{\theta}\left( x^{\left( i \right)} \right) \right) + \left( 1-y^{\left( i \right)} \right)log\left( 1-h_{\theta}\left( x^{\left( i \right)} \right) \right) \right] \right\} + \frac{\lambda}{2m} \sum_{j=1}^{2} \theta_{j}^{2}$ 因为其 $\frac{\partial J(\theta)}{\partial \theta}$ 的形式与上面线性回归一样，所以梯度下降的过程同上。 课程资料 week3课程讲义 编程作业ex2]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning (Week2)]]></title>
    <url>%2Fposts%2FMachine%20Learning%20(Week2)%2F</url>
    <content type="text"><![CDATA[在第二周的课程里，主要讲了 多变量线性回归以及相应的 梯度下降实践，一些梯度下降的技巧如 学习速率的选择， Feature Scaling - 特征缩放等，最后介绍了 Polynomial Regression - 多项式回归和 Normal Equation - 正规方程。 Linear Regression with multiple variables - 多变量线性回归 Multiple Feature - 多变量 参考 week1 第二节内容。 Gradient Descent for multiple variable - 多变量梯度下降 参考 week1 第三节内容。 Gradient Descent in practise 1: Feature Scaling - 梯度下降实践1：特征缩放 首先还是用房价预测回归的例子来说明： 假设我们有两个特征： $x_{1} = size(feet_{2}) \in (0,2000)$ $x_{2} = number of bedrooms \in (1,5)$ 可以看到 $x_{1}$ 比 $x_{2}$ 的取值范围要大了几个数量级。这么做的直接后果就是，只要 $x_{1}$ 的参数 $\theta_{1}$ 稍微变化一下，预测值 $h_{\theta}(x)$ 与实际价格之间的误差就会偏移得很厉害，也就导致 $J(\theta)$ 在 $\theta_{1}$ 偏移得很厉害，如下图所示： 我们看到，假如对此进行梯度下降，须迭代多次才能达到极值点。显然这是不ok的。 所以我们要对 $x_{1}$ 和 $x_{2}$ 进行缩放，让他们的取值范围落在同一个区间，或者相近区间。 一般我们会用下面的方法进行缩放： $x_{1} = \frac{size(feet_{2})}{2000}$ $x_{2} = \frac{number of bedrooms}{5}$ 这样我们就能令 $x_{1},x_{2} \in [0,1]$ ，这种方法也叫 归一化。 我们在做梯度下降时，速度就会快很多： 更普遍的，我们采用如下方法： $x := \frac{x-\mu}{s}$ 其中$\mu$为$x$的均值，$s$为$x$的方差。这样就能令$x\in (-1,1)$。 Gradient Descent in practise 2: Learning rate - 梯度下降实践2：学习速率 如何选择学习速率是做梯度下降的很关键的一步。在 week1 第三节的内容里，我们了解到学习速率$\alpha$设置不当会有怎样的结果。所以当我们做梯度下降时一定要确保每一次迭代时$J(\theta)$都在 减小，最后收敛于某个值。一般我们可以作 迭代次数 - $J(\theta)$图来观察$J(\theta)$是否收敛： 收敛 发散 或者设置某个阈值（比如$0.001$）来检测， 当$J(\theta)$ 减小的差值小于阈值时，可以认为$J(\theta)$已收敛到极值。 Features and Polynomial Regression - 特征与多项式回归 线性回归顾名思义，用于特征与结果有明显的线性关系时的情况。假如我们的某些特征与结果是非线性关系的，比如下图，我们只要观察散点的分布趋向哪种多项式函数然后做拟合就好了： Normal Equation - 正规方程 根据 week1 第三节最后的内容，我们得到： $$\frac{\partial J}{\partial \theta} = grad_{(n+1)\times 1} = \frac{1}{m} X^{T}(X\theta - Y)$$ 令其为$0$，可得： $$X^{T}X\theta - X^{T}Y = 0$$ $X^{T}X\theta = X^{T}Y$ 则得： $\theta = (X^{T}X)^{-1}X^{T}Y$ 其中要注意的是，$X^{T}X$不一定是可逆的。比如 特征数过多$(m&lt;&lt;n)$，此时$r(X)\leq m$，$r(X^{T}X)\leq min(r(X),r(X^{T}))\leq m &lt; n$。而$X^{T}X\in n\times n$，因此$X^{T}X$是不可逆的。 因此在 样本数量远大于特征数量时才能用正规方程。 小结 具体问题具体分析，梯度下降虽然基础，但在许多问题上仍然是很有效的。正规方程用在合适的问题上则能非常快的得出结果。各司其职。 课程资料 week2课程讲义 编程作业ex1]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning.ai 1 - Neural Networks and Deep Learning (week1)]]></title>
    <url>%2Fposts%2Fdeeplearning.ai%201%20-%20Neural%20Networks%20and%20Deep%20Learning%20(week1)%2F</url>
    <content type="text"><![CDATA[第一周的课程里主要是简单介绍 deeplearning - 深度学习， Neural NetWork - 神经网络的概念。 Introduction to Deep Learning - 深度学习简介 What is Neural Network? - 什么是神经网络？ 其实 神经网络并没有想象中那么复杂，它由三部分组成： 输入层 隐藏层 输出层 其中每一层都包含数个 neuron - 神经元，我称之为 计算节点。每一层的每一个节点都完成一种计算任务，上一层的节点计算的结果会作为当前层的节点的输入，当前层节点的输出则作为下一层节点的输入。 我们用一个简单的例子来解释： 假如现在有一组数据，是关于房子面积和价格的表格，如下： $Size in feet^{2}$ (x) $prize$ (y) 2104 460 1416 232 1534 315 852 178 我们可以作线性回归，可能画出来的图是这样的： 那么我们可以作如下定义： 输入层节点为房子面积$(x)$ 隐藏层节点为 蓝色直线所代表的 拟合函数 $(f)$ 输出层节点为 拟合函数在对应输入上的输出$(\hat{y})$ 可作下图： graph LR; x((x)) --> f((f)); f((f)) --> y((yhat)); 这就是最简单的神经网络，它只有$3$层，每层只有$1$个节点。只要我们将随便一个房屋子的面积作为输入丢进去，它就会帮我们预测输出价格。 可是只靠面积来预测价格肯定是不严谨的，可能会有多个因素，比如地段，交通便利情况等。这时我们就会有多个输入，也就意味着输入层节点有多个，与输入的特征一一对应。 我的总结， 神经网络就是一个多层的嵌套函数。这句话会在后面的笔记得以充分体现。 Supervised Learning with Neural Networks - 有监督学习神经网络 有监督学习，简要的说，就是我们有一个 数据集，并且我们知道这对数据的 输出是张什么样的，而且可以肯定作为变量的输入数据与作为结果的输出数据之间有着必然的联系。 有监督学习通常分为 regression - 回归与 classification - 分类两类问题。 对于回归问题，一般我们的数据都是 Structured - 结构化 的，有着明显的特征： 对于分类问题，数据一般是 Unstructured - 无结构化的，特征不明显： 关于神经网络的发展请阅读课程资料，这里不在赘述。 课程资料 week1课程讲义]]></content>
      <categories>
        <category>deeplearning.ai笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>数学</tag>
        <tag>深度学习</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[deeplearning.ai 0 (openning)]]></title>
    <url>%2Fposts%2Fdeeplearning.ai%200%20(openning)%2F</url>
    <content type="text"><![CDATA[开篇的话 大约上个月，在 Coursera上申请 Andrew Ng的 deeplearning.ai系列课程助学金通过了。想着偏偏在我复习考研的时间段里开课，又不想转换班次，于是“狠下心来”开始上课，这下负担又重了，不仅要复习，还要兼顾Coursera上两门重量级课程，虽然有做笔记和推导，但是实在没有时间再去做整理发布了。 那么为什么会有这个开篇呢？因为我实在忍不住。今天刚上完 系列课程1–Neural Networks and Deep Learning的 week3课程，而 Machine Learning那边也进行到 week6。因为在 Machine Learning里也教到 Back Propagation - 反向传播算法后面一点的内容，同样的 deeplearning那边也讲到，感觉对 BP有了非常清晰的思路。迫不及待想整理上来。趁着国庆做一次详细整理吧。 以下是一些关于此课程笔记的事项： 这里记录了我在Coursera上学习deeplearning.ai专项课程的笔记 同时会把课程相关资料整理一并发布，包括视频，lecturenote，homework，和我的答案解释。 课程链接:可以申请助学金以免费听课，需15个工作日处理。]]></content>
      <categories>
        <category>deeplearning.ai笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>数学</tag>
        <tag>深度学习</tag>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning (Week1)]]></title>
    <url>%2Fposts%2FMachine%20Learning%20(Week1)%2F</url>
    <content type="text"><![CDATA[在第一周的课程里简要介绍了 什么是机器学习， Model - 模型和 Cost Function - 代价函数的概念，以及一些必要的 线性代数的知识。 特别声明：这里不会对线性代数基础进行记录，有需要了解的请自行学习。 敬请留意 Introduction - 简介 What is Machine Learning? - 何为机器学习？ 引用Tom Mitchell的经典解释（有点像绕口令） Two definitions of Machine Learning are offered. Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition. Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” Example: playing checkers. E = the experience of playing many games of checkers T = the task of playing checkers. P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning and Unsupervised learning. Supervised Learning - 有监督学习 有监督学习，简要的说，就是我们有一个 数据集，并且我们知道这对数据的 输出是张什么样的，而且可以肯定作为变量的输入数据与作为结果的输出数据之间有着必然的联系。 有监督学习通常分为 regression - 回归与 classification - 分类两类问题。 在回归问题中，我们要做的就是得到一个连续的 预测函数去拟合离散的数据，也就是要把 输入变量映射到连续函数上，从而实现未知数据的预测。 例子： Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. We could turn this example into a classification problem by instead making our output about whether the house “sells for more or less than the asking price.” Here we are classifying the houses based on price into two discrete categories. 在分类问题中，输出结果是离散的，比如二分类问题，每一类别分别对应0,1两个离散数值，我们要做的就是得到一个 预测函数，能够根据输入变量得到离散的输出结果，也就是要把 输入变量映射到离散的类别中 例子： (a) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. (b) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture Unsupervised Learning - 无监督学习 无监督学习就是有监督学习的反面情况，即我们有一组数据集，但我们并不知道它的输出结果是什么，或者它根本就没有输出，甚至它本身代表什么我们都不知道。我们要做的就是从这堆数据中 找出它的规律或者结构，来确定这些输入变量产生的影响，比如将这堆数据分组。 特别的是，无监督学习并不像有监督学习那样有基于预测结果的反馈。 现实生活中有很多这样的例子，比如你有一堆新闻内容的数据，你要把有关联的分成一组。像这样的算法叫做 聚类，就如字面意思一样。 例子： Model and Cost Function - 模型与代价函数 Model Representation - 模型的表示方法 对于 有监督学习，在这门课程里有一套专门的符号，参数，公式的表示方法： $vector$: 向量（都指列向量） $m$: 训练样本组数 $x^{\left(i \right)}$ : 第$i$组输入变量（一般为向量） $y^{\left(i \right)}$: 第$i$组输出变量（一般为向量） $\left(x^{\left(i \right)}, y^{\left(i \right)} \right)$: 第$i$组训练样本 $\left(x, y\right)$: 全体训练样本数据 $X$: 输入变量空间（一般为矩阵） $Y$: 输出变量空间（一般为矩阵） $h_{\theta} \left(x \right)$: 预测函数 $\theta_{j}$: 第$j$组学习参数 例子： 假设我们有一组（房子面积, 价格）数据集，对应下表: $Size in feet^{2}$ (x) $prize$ (y) 2104 460 1416 232 1534 315 852 178 其中 $m = 4$ $x^{\left(1 \right)} = 2104$ $y^{\left(1 \right)} = 460$ $\left(x^{\left(1 \right)}, y^{\left(1 \right)} \right) = (2104, 460)$ $X$ = $\begin{bmatrix}2104 &amp; 1416 &amp; 1534 &amp; 852\end{bmatrix}^{T}$ $Y$ = $\begin{bmatrix}460 &amp; 232 &amp; 315 &amp; 178\end{bmatrix}^{T}$ $h_{\theta}\left(x\right)=\theta_{0}+\theta_{1}x$ $\theta_{1}$: 第$1$组学习参数 对于多变量（或者叫 feature - 特征）的表示方法，如下 例子： 假设我们有一组多个特征的数据，每组特征对应一个确定的输出： $X_{0}$ $X_{1}$ $X_{2}$ $X_{3}$ … $X_{n}$ $Y$ $1$ $x_{1}^{(1)}$ $x_{2}^{(1)}$ $x_{3}^{(1)}$ … $x_{n}^{(1)}$ $y^{(1)}$ $1$ $x_{1}^{(2)}$ $x_{2}^{(2)}$ $x_{3}^{(2)}$ … $x_{n}^{(2)}$ $y^{(2)}$ $1$ $x_{1}^{(3)}$ $x_{2}^{(3)}$ $x_{3}^{(3)}$ … $x_{n}^{(3)}$ $y^{(3)}$ … … … … … … … $1$ $x_{1}^{(m)}$ $x_{2}^{(m)}$ $x_{3}^{(m)}$ … $x_{n}^{(m)}$ $y^{(m)}$ 其中 $X_{m\times (n+1)} = \begin{bmatrix}1 &amp; x_{1}^{(1)} &amp; \cdots &amp;x_{n}^{(1)} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{1}^{(m)} &amp; \cdots &amp; x_{n}^{(m)} \\ \end{bmatrix}$ $Y_{m\times 1} = \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)}\end{bmatrix}^{T}$ $\theta = \begin{bmatrix} \theta_{0} &amp; \theta_{1} &amp; \cdots &amp; \theta_{n} \end{bmatrix}^{T}$ $h_{\theta}\left(x\right)=\begin{bmatrix}h_{\theta}\left(x^{(1)}\right)&amp;h_{\theta}\left(x^{(2)}\right)&amp;\cdots&amp;h_{\theta}\left(x^{(m)}\right)\end{bmatrix}^{T}=\begin{bmatrix}\theta^{T}x^{(1)}&amp;\theta^{T}x^{(2)}&amp;\cdots&amp;\theta^{T}x^{(m)}\end{bmatrix}^{T}$ 整个监督学习的过程，就是找到最优的$\theta$，从而得到最优的 $h_{\theta}\left(x\right)$ 。对于 回归问题，预测就是我们把一组$x$丢进 $h_{\theta}\left(x\right)$ 中，得到的结果就是预测值。对于 分类问题，$h_{\theta}\left(x\right)$得到的结果是一个概率，即输入$x$属于某一类的概率值是多少。或许你觉得我解释得很抽象，因为这里的内容只是让你的大脑对机器学习有一个大致的轮廓。详细的过程将记录在后面的笔记中，请读者放心。 Cost Function - 代价函数 回到我们上面的那个（房子面积, 价格）数据集的例子中。这是一个 单变量的回归问题。如下 在这里，我们的预测函数为 $h_{\theta}(x)=\theta_{0}+\theta_{1}x$ 。当$\theta$取不同值时，对应如下图： 我们要找到最优的$\theta$去拟合$(x,y)$，首先就要定义一个能判断当前$\theta$是否最优的函数，这个函数就是 代价函数。在这里我们将它定义为$J(\theta)$。 那么它等于什么呢？下面给个直观的图例辅助解释： 在这幅图里，有3组样本数据为 $X$ $Y$ 1 1 2 2 3 3 分别对应上图三个 红色×点。 黑色斜线为 $h_{\theta}(x)=0+0.5x$ 过这3点分别作垂直于$x$轴的 垂线段交于 $h_{\theta}(x)$ 。则第$i$个样本点的误差（即代价）就是该样本点对应 垂线段的长度，为 $\left|h_{\theta}(x^{(i)})-y^{(i)}\right|$ 为方便处理，我们将绝对值去掉，重新定义误差为 $\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2}$ 则总误差为 $\sum_{i=1}^{3}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2}$ 平均误差为 $\frac{1}{3}\sum_{i=1}^{3}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2}$ 为了方便后面处理，这里我们一般将平均误差乘一个$\frac{1}{2}$，即 $\frac{1}{2 \times 3}\sum_{i=1}^{3}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2}$ 上式就是我们的代价函数，即 $J(\theta)=\frac{1}{6}\sum_{i=1}^{3}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2}$ 将上式扩展到$m$个样本点的一般情况，即 $J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2}$ 当我们的$\theta$能令$J(\theta)$取到最小值时，我们就认为这是最优的$\theta$。 是不是很直观？ 有了代价函数之后，我们要做的就是找出令它取得最小值的$\theta$，下图就是我们的任务： Parameter Learning - 参数学习 Gradient Descent - 梯度下降 这是典型的极值问题，在数学方法中，我们可以求导解决，可是在计算机程序中，我们要用一种 通用的数值方法，去逼近。 我们看只有一个学习参数的情况，假设 $h_{\theta}(x)=\theta x$ 当 $\theta$ 比较小时： 我们看到 $h_{\theta}(x)$ 没有很好地拟合数据， $J(\theta)$ 比较大。 当 $\theta$ 比较大时： 同样的， $h_{\theta}(x)$ 没有很好地拟合数据， $J(\theta)$ 比较大。 当 $\theta$ 比取到能使 $h_{\theta}(x)=\theta x$ 很好地拟合数据时： 这时的 $\theta$ 就是 $J(\theta)$ 的极小值点。也就是最优的 $\theta$ 。 接下来我们就来讲，如何让计算机自动训练出最优的$\theta$ 我们继续用上面的例子， 当 $\theta$ 比较小时： 我们求得 $J(\theta)$ 在当前 $\theta$ 的导数， 小于 $0$ 。此时我们把 $\theta$ 更新为 $\theta - \alpha\frac{dJ(\theta)}{d\theta}$ ， $\theta$ 就会 变大，往极值点靠近。其中 $\alpha$ 为 学习速率。 当 $\theta$ 比较大时： 我们求得 $J(\theta)$ 在当前 $\theta$ 的导数， 大于 $0$ 。此时我们把 $\theta$ 更新为 $\theta - \alpha\frac{dJ(\theta)}{d\theta}$ ， $\theta$ 就会 减小，往极值点靠近。其中 $\alpha$ 为 学习速率。 这就是 梯度下降算法。通过多次的迭代，更新 $\theta$ ，我们就能无限逼近最优值。 将 $\theta$ 拓展到 二维向量（即有两个参数）的情形，我们可能会得到如下的 $J(\theta)$ ： 这是一个二维曲面，这种情况我们就要分别对 $\theta_{0},\theta_{1}$ 求偏导来进行梯度下降。 对于梯度下降，还有一些要注意的地方： 关于 学习速率 $\alpha$ ，怎样设置学习速率也是很关键的问题，如果 $\alpha$ 设置的 过小，则梯度下降就会收敛得很慢，训练时间会过长。如果 $\alpha$ 设置的过大，则梯度下降有可能会发散，就是越过了极值点： 所以我们在做迭代时一定要关注着$J(\theta)$，确保它是在下降的。 在实际问题中，我们的 $J(\theta)$ 一般不会是 凸函数，也就是说我们做梯度下降得到的只是 局部最优值，而不是 全局最优值： Gradient Descent for Liner Regression - 线性回归中的梯度下降 对于线性回归，我们有如下定义： $X_{m\times (n+1)} = \begin{bmatrix}1 &amp; x_{1}^{(1)} &amp; \cdots &amp;x_{n}^{(1)} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp; x_{1}^{(m)} &amp; \cdots &amp; x_{n}^{(m)} \\ \end{bmatrix}$ $Y_{m\times 1} = \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)}\end{bmatrix}^{T}$ $\theta = \begin{bmatrix} \theta_{0} &amp; \theta_{1} &amp; \cdots &amp; \theta_{n} \end{bmatrix}^{T}$ $h_{\theta}\left(x\right)=\begin{bmatrix}h_{\theta}\left(x^{(1)}\right)&amp;h_{\theta}\left(x^{(2)}\right)&amp;\cdots&amp;h_{\theta}\left(x^{(m)}\right)\end{bmatrix}^{T}=\begin{bmatrix}\theta^{T}x^{(1)}&amp;\theta^{T}x^{(2)}&amp;\cdots&amp;\theta^{T}x^{(m)}\end{bmatrix}^{T} = X \theta$ $h_{\theta}\left(x^{(i)}\right) = \theta_{0} + \theta_{1}x_{1}^{(i)} + \cdots + \theta_{n}x_{n}^{(i)}$ $J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}\left(h_{\theta}(x^{(i)})-y^{(i)}\right)^{2}$ 我们对 $J(\theta)$ 求所有 $\theta$ 的偏导： $\frac{\partial J}{\partial \theta_{j}} = \frac{1}{m} \sum_{i=1}^{m} \left[\left( h_{\theta}(x^{(i)})-y^{i} \right) \frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_{j}} \right]$ 当 $j=0$ 时： $\frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_{0}} = 1$ 当 $j=1 \cdots n$ 时： $\frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_{j}} = x_{j}^{(i)}$ 综上： $\frac{\partial J}{\partial \theta_{0}} = \frac{1}{m} \sum_{i=1}^{m} \left[\left( h_{\theta}(x^{(i)})-y^{(i)} \right)\right]$ $\frac{\partial J}{\partial \theta_{1}} = \frac{1}{m} \sum_{i=1}^{m} \left[\left( h_{\theta}(x^{(i)})-y^{(i)} \right) x_{1}^{(i)} \right]$ $\vdots$ $\frac{\partial J}{\partial \theta_{n}} = \frac{1}{m} \sum_{i=1}^{m} \left[\left( h_{\theta}(x^{(i)})-y^{(i)} \right) x_{n}^{(i)} \right]$ 更新 $\theta$ ： $\theta_{0}:=\theta_{0} - \alpha \frac{\partial J}{\partial \theta_{0}} = \theta_{0} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left[\left( h_{\theta}(x^{(i)})-y^{(i)} \right)\right]$ $\theta_{1}:=\theta_{1} - \alpha \frac{\partial J}{\partial \theta_{1}} = \theta_{1} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left[\left( h_{\theta}(x^{(i)})-y^{(i)} \right) x_{1}^{(i)} \right]$ $\vdots$ $\theta_{n}:=\theta_{n} - \alpha \frac{\partial J}{\partial \theta_{n}} = \theta_{n} - \alpha \frac{1}{m} \sum_{i=1}^{m} \left[\left( h_{\theta}(x^{(i)})-y^{(i)} \right) x_{n}^{(i)} \right]$ 我们将上述过程向量化： 首先将偏导数向量化： $$\frac{\partial J}{\partial \theta_{0}} = \frac{1}{m} \begin{bmatrix}1&amp;1&amp;\cdots&amp;1\end{bmatrix} \begin{bmatrix}h_{\theta}(x^{(1)})-y^{(1)}\\h_{\theta}(x^{(2)})-y^{(2)}\\ \vdots\\h_{\theta}(x^{(m)})-y^{(m)}\end{bmatrix}$$ $$\frac{\partial J}{\partial \theta_{1}} = \frac{1}{m} \begin{bmatrix}x_{1}^{(1)}&amp;x_{1}^{(2)}&amp;\cdots&amp;x_{1}^{(m)}\end{bmatrix} \begin{bmatrix}h_{\theta}(x^{(1)})-y^{(1)}\\h_{\theta}(x^{(2)})-y^{(2)}\\ \vdots\\h_{\theta}(x^{(m)})-y^{(m)}\end{bmatrix}$$ $$\vdots$$ $$\frac{\partial J}{\partial \theta_{n}} = \frac{1}{m} \begin{bmatrix}x_{n}^{(1)}&amp;x_{n}^{(2)}&amp;\cdots&amp;x_{n}^{(m)}\end{bmatrix} \begin{bmatrix}h_{\theta}(x^{(1)})-y^{(1)}\\h_{\theta}(x^{(2)})-y^{(2)}\\ \vdots\\h_{\theta}(x^{(m)})-y^{(m)}\end{bmatrix}$$ 可得： $$\frac{\partial J}{\partial \theta} = grad_{(n+1)\times 1} = \begin{bmatrix}\frac{\partial J}{\partial \theta_{0}}\\\frac{\partial J}{\partial \theta_{1}}\\ \vdots\\\frac{\partial J}{\partial \theta_{n}}\end{bmatrix} = \frac{1}{m} X^{T}(X\theta - Y)$$ 接着将梯度下降的过程向量化： $$\theta := \theta - \alpha \frac{1}{m} X^{T}(X\theta - Y)$$ PS:其实上面的 多变量线性回归梯度下降 是 week2的内容，因为不算太复杂我就搬到这里讲了，那么 week2的笔记里就会跳过这部分内容，请大家注意。 课程资料 week1课程讲义]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning (Week0) Openning]]></title>
    <url>%2Fposts%2FMachine%20Learning%20(Week0)%20Openning%2F</url>
    <content type="text"><![CDATA[开篇的话 这里记录了我在Coursera上听吴恩达机器学习课程的笔记，以表示我是有学习的:) 同时会把课程相关资料整理一并发布，包括lecturenote，homework，和我的答案解释。 课程链接:可以直接注册听课，但是不会有证书。 特别声明：笔记在一些对类似什么是机器学习，机器学习的应用这些字面意义上的概念只会一笔带过，需要了解的自行谷歌或百度，笔记之着重于机器学习本身的内容，算法，推导，在一些理论细节上可能会做详细深入。 敬请留意]]></content>
      <categories>
        <category>机器学习笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>ML</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发布测试]]></title>
    <url>%2Fposts%2F%E5%8F%91%E5%B8%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[博客发布测试 2级标题 123 from bs4 import BeautifulSoupdef func(args): return args Content (md partial supported) Content (md partial supported) Content (md partial supported) Content (md partial supported) Content (md partial supported) Content (md partial supported) Content (md partial supported) $$n\cdot m\cdot \lg b$$ Simple inline $a = b + c$. $$\frac{\partial u}{\partial t} = h^2 \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} + \frac{\partial^2 u}{\partial z^2}\right)$$ $$J\left(\theta\right) = -\left[\frac{1}{m}\sum_{i=1}^{m}y^\left(i\right)log\left(h_{\theta}\left(x^{\left(i\right)}\right)\right)+ \left(1-y^{\left(i\right)}\right)log\left(1-h_{\theta}\left(x^{\left(i\right)}\right)\right)\right]+ \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$$ $Size in feet^{2}$ (x) $prize$ (y) 2104 460 1416 232 1534 315 852 178 graph LR; a11(("a1[0](i)")) --> a12(("a1[1](i)")); a11(("a1[0](i)")) --> a22(("a2[1](i)")); a11(("a1[0](i)")) --> a32(("a3[1](i)")); a21(("a2[0](i)")) --> a12(("a1[1](i)")); a21(("a2[0](i)")) --> a22(("a2[1](i)")); a21(("a2[0](i)")) --> a32(("a3[1](i)")); a31(("a3[0](i)")) --> a12(("a1[1](i)")); a31(("a3[0](i)")) --> a22(("a2[1](i)")); a31(("a3[0](i)")) --> a32(("a3[1](i)")); a12(("a1[1](i)")) --> y(("a1[2](i)")); a22(("a2[1](i)")) --> y(("a1[2](i)")); a32(("a3[1](i)")) --> y(("a1[2](i)")); y(("a1[2](i)")) --> yh((yhat)); graph TB; subgraph L0; a11(("a1[0](i)")) --- a21(("a2[0](i)")); a21(("a2[0](i)")) --- a31(("a3[0](i)")); end;]]></content>
      <categories>
        <category>回收站</category>
      </categories>
      <tags>
        <tag>博客测试</tag>
      </tags>
  </entry>
</search>
