---
title: Machine Learning (Week4)
date: 2018-01-26 12:29:41
categories: 机器学习笔记
tags: [机器学习, ML, 数学]
mathjax: true
comments: true
---
第四周的课程开始介绍 __Neural Networks - 神经网络__ 的概念，包括它的 __结构__ 和 __表示方法__。

{% asset_img pic0.jpg %}

<!--more-->

# Neural Networks: Representation - 神经网络：表述
## Non-linear hypotheses - 非线性假设
我们之前所讲到的，线性回归和逻辑回归，它们在进行 __非线性预测和分类__ 时，在 __特征比较少__ 的情况下，用 __特征多项式__ 来训练会有不错的表现（如下式，两个特征）。
<center>
{% math %}
\theta_{0} + \theta_{1}x_{1}^{2} + \theta_{2}x_{1}x_{2} + \theta_{3}x_{2}^{2}
{% endmath %}
</center>
<br>

但是一旦特征变多，比如几十甚至上百个时，利用特征多项式训练，所组合出来的新特征将会是一个非常庞大的数字（如下式，100个特征，仅两两组合）。这无疑是行不通的。
<center>
{% math %}
\theta_{0} + \theta_{1}x_{1}^{2} + \theta_{2}x_{2}^{2} + \theta_{3}x_{3}^{2} + \cdots + \theta_{100}x_{100}^{2} + \theta_{101}x_{1}x_{2} + \theta_{102}x_{1}x_{3} + \cdots + \theta_{10000}x_{99}x_{100}
{% endmath %}
</center>
<br>

所以，我们需要一种新的模型，就是 __神经网络__。

## Neurons and the brain - 神经元和大脑
><font color="#aeaeae">这部分是介绍神经网络的一些背景，若不感兴趣可</font>[点此跳至下一节](#Model-representation-I-模型表示-1)

首先神经网络是模仿大脑结构来建立的。我们大脑可以学习很多事情，小到一个行为，一个动作，大到一门学科，一门语言，如果我们想让计算机来处理不同的学习任务，似乎我们需要针对性的编写不同的程序来实现。可是人脑在学习的时候真的用了这么多不同的“算法”么？我们能不能假设其实大脑只有一种学习算法，但却可以处理很多的事情？下面我们来看一下关于这个假设的一些证据。

* 科学家将小白鼠的 __耳朵__ 到 __听觉皮层__ 的 __听觉神经__ 剪断，然后将 __视觉神经__ 接到 __听觉皮层__ 上，结果，__听觉皮层__ 学会了 __看__。
{% asset_img pic1.png %}

* 美国食品和药物管理局在临床实验一个名为brainport的系统，能帮助失明人士 __看见__ 东西。它是这样工作的：在失明人士头上带一个灰度摄像头，获取面前场景的低分辨率灰度图像，然后将信号连接到 __舌头__ 上的一个很薄的 __电极阵列__ 上，这样 __每个像素点都能映射到舌头的某个位置__，这种系统能让人在几十分钟内用舌头学会 __看东西__。
{% asset_img pic2.png %}

这些例子说明，我们大脑的每一块区域，都能处理不同的信息，图像，声音，触觉，嗅觉等等，就好比一个机器可以接受任何传感器输入的信号。如果我们找出 __大脑的学习方法__，然后在计算机上实现，这也许是真正的迈向人工智能。而 __神经网络__ 则是第一步。

下面我们将深入到神经网络的细节。

## Model representation I - 模型表示 1
每一个神经元，也可以叫一个 __计算节点__，它接受来自 __前一个神经元的输出__ 作为 __它的输入__，然后经过 __计算__，将 __输出__ 送给 __下一个神经元__ 作为其 __输入__。

{% mermaid %}
graph LR;
    a1((a1)) -- input --> a2((a2));
    a2((a2)) -- output --> a3((a3));
{% endmermaid %}   

比如一个以 __逻辑回归__ 为模型的网络可以表示成这样：
{% asset_img pic3.png %}

这是一个只有 __两层__ 的神经网络，只包含 __输入层__ 和 __输出层__。

下面我们看一个3层的神经网络：
{% asset_img pic4.png %}

其中layer 2是 __隐藏层__，是中间的计算单元，将结果反馈到下一层。这里我们定义一下符号表示：
* {% math %}a_{i}^{(j)}{% endmath %}为第j层第i个节点
* {% math %}h_{\Theta}(x){% endmath %}为输出结果